{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "L4",
      "authorship_tag": "ABX9TyO4BFBllL+Qrjqq6ysmG055",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/powervnc/RAG_Jane_Austen/blob/main/RAG_with_Jane_Austen_Novels.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# INSTALLING LIBRARIES"
      ],
      "metadata": {
        "id": "Z465q4ixuhsk"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6jEeHnaM30bA"
      },
      "outputs": [],
      "source": [
        "! pip install langchain_community tiktoken langchain-openai langchainhub chromadb langchain"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install python-dotenv"
      ],
      "metadata": {
        "id": "whJ53MBD6L3o",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9ee5c852-6f97-4f9b-9d00-30a73b8ae85a"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: python-dotenv in /usr/local/lib/python3.12/dist-packages (1.1.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- langchain community -> we wil use it for OpenAiEmbeddings\n",
        "- tiktoken -> using for splitting, tokenization\n",
        "- langchainhub -> for prompt template\n",
        "- chromadb -> LOCAL vector database"
      ],
      "metadata": {
        "id": "2uF-HRRI4cTm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# SETTING UP THE ENV KEYS\n"
      ],
      "metadata": {
        "id": "p_KP2fFYubM2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile .env\n",
        "OPENAI_API_KEY=\n",
        "LANGSMITH_TRACING=true\n",
        "LANGSMITH_API_KEY="
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u-mo30H6-xJS",
        "outputId": "2839ed12-7bc0-4f25-a373-8965bed255fd"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting .env\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from dotenv import load_dotenv\n",
        "import os\n",
        "\n",
        "load_dotenv()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XUvVX14w-IEr",
        "outputId": "d1ebe557-fa6c-4063-f43d-3e7d1c917bb5"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# DOWNLOADING THE JANE AUSTEN NOVELS FROM PROJECT GUTENBERG"
      ],
      "metadata": {
        "id": "EyaSsvf24djT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "novels_info = {\n",
        "    \"sense_and_sensibility\": \"https://www.gutenberg.org/files/161/161-0.txt\",\n",
        "    \"pride_and_prejudice\": \"https://www.gutenberg.org/files/1342/1342-0.txt\",\n",
        "    \"emma\": \"https://www.gutenberg.org/files/158/158-0.txt\"\n",
        "}\n"
      ],
      "metadata": {
        "id": "yNZu3m7s4abu"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pathlib import Path\n",
        "\n",
        "novels_folder = Path(\"novels\")\n",
        "novels_folder.mkdir(exist_ok=True)\n"
      ],
      "metadata": {
        "id": "0sLN9Q0J4vdl"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "\n",
        "MAX_TRIES = 5\n",
        "\n",
        "for name, url in novels_info.items():\n",
        "  complete_path = novels_folder/f\"{name}.txt\"\n",
        "  if not complete_path.exists():\n",
        "    for i in range(MAX_TRIES):\n",
        "        try:\n",
        "            print(f\"Downloading {name}\")\n",
        "            req = requests.get(url)\n",
        "            req.raise_for_status() #checks status of https request\n",
        "            complete_path.write_text(req.text, encoding=\"utf-8\")\n",
        "            print(f\"Finished downloading {name}\")\n",
        "            break\n",
        "        except requests.exceptions.RequestException as e:\n",
        "          print(f\"Error downloading {name}: {e}\")\n",
        "          if i==MAX_TRIES:\n",
        "            print(f\"Reached maximum downloading tried for {name}\")\n",
        "  else:\n",
        "    print(f\"Already downloaded {name}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2wms2Uuh5PzH",
        "outputId": "e68d954e-1486-41ab-cadb-876cc3016f5f"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Already downloaded sense_and_sensibility\n",
            "Already downloaded pride_and_prejudice\n",
            "Already downloaded emma\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# CREATING THE CORRESPONDING DATABASES FOR THE NOVELS\n"
      ],
      "metadata": {
        "id": "AcAd2w8F72sC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.document_loaders import TextLoader\n",
        "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
        "from langchain_community.vectorstores import Chroma\n",
        "from langchain_openai import OpenAIEmbeddings\n",
        "\n",
        "\n",
        "text_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(\n",
        "    chunk_size = 300,\n",
        "    chunk_overlap = 50\n",
        ")\n",
        "\n",
        "vectorstores = {}\n",
        "\n",
        "for name in novels_info.keys():\n",
        "  loader = TextLoader(str(novels_folder / f\"{name}.txt\"), encoding=\"utf-8\")\n",
        "  docs = loader.load()\n",
        "  splits = text_splitter.split_documents(docs)\n",
        "  vectorstores[name] = Chroma.from_documents(\n",
        "      documents = splits,\n",
        "      embedding = OpenAIEmbeddings(),\n",
        "      collection_name=name\n",
        "  )\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "06oKM2-x781F"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vectorstores"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Hka4sjly-HKS",
        "outputId": "f6a97e7d-df3e-479a-a4dd-749d95d00385"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'sense_and_sensibility': <langchain_community.vectorstores.chroma.Chroma at 0x7da1d430e6f0>,\n",
              " 'pride_and_prejudice': <langchain_community.vectorstores.chroma.Chroma at 0x7da1d430fd70>,\n",
              " 'emma': <langchain_community.vectorstores.chroma.Chroma at 0x7da1d430fd40>}"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ROUTING TO THE APPROPIATE DATABASE"
      ],
      "metadata": {
        "id": "38X-Sx82--UU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import Literal\n",
        "\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from pydantic import BaseModel, Field\n",
        "from langchain_openai import ChatOpenAI\n",
        "\n",
        "\n",
        "#restricting the output of the querry\n",
        "class RouteQuery(BaseModel):\n",
        "    datasource: Literal[\"emma\", \"pride_and_prejudice\", \"sense_and_sensibility\"] = Field(\n",
        "        ...,\n",
        "        description=\"Given a user question choose which datasource(novel) would be most relevant for answering their question\",\n",
        "    )\n",
        "\n",
        "llm = ChatOpenAI(model=\"gpt-3.5-turbo-0125\", temperature=0)\n",
        "llm = llm.with_structured_output(RouteQuery)\n",
        "\n",
        "system = \"You are an expert at routing a user question to the appropriate data source. Based on the programming language the question is referring to, route it to the relevant data source.\"\n",
        "\n",
        "\n",
        "prompt = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        (\"system\", system),\n",
        "        (\"human\", \"{question}\"),\n",
        "    ]\n",
        ")\n",
        "\n",
        "router = prompt | llm\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nlMVQeFi_P6b",
        "outputId": "82f5297b-9998-47a3-d9b4-e7ce1bff6ca3"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/langchain_openai/chat_models/base.py:1928: UserWarning: Cannot use method='json_schema' with model gpt-3.5-turbo-0125 since it doesn't support OpenAI's Structured Output API. You can see supported models here: https://platform.openai.com/docs/guides/structured-outputs#supported-models. To fix this warning, set `method='function_calling'. Overriding to method='function_calling'.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "question = \"What is Elizabeth's opinions about love and courtship\"\n",
        "result = router.invoke({\"question\": question})\n",
        "\n",
        "result"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-JP2KDkEA7Yg",
        "outputId": "050293e4-3f5e-4b2f-e2f0-954b37c5696e"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "RouteQuery(datasource='pride_and_prejudice')"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "result.datasource"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "ehWtQgXUBodN",
        "outputId": "f74c0765-02f3-4f93-f52f-e346b7798e85"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'pride_and_prejudice'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def chose_retriever(datasource):\n",
        "  retriever = None\n",
        "  if datasource in ['pride_and_prejudice', 'emma', 'sense_and_sensibility']:\n",
        "    retriever = vectorstores[datasource].as_retriever()\n",
        "  elif datasource == 'emma':\n",
        "    retriever = vectorstores['pride_and_prejudice'].as_retriever()\n",
        "  return retriever\n"
      ],
      "metadata": {
        "id": "b5bwLScYBtwt"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "retriever = chose_retriever(result.datasource)"
      ],
      "metadata": {
        "id": "PeI1phWlCq41"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# MULTI QUERY APPOACH TO QUERRY TRANSLATION\n",
        "see template"
      ],
      "metadata": {
        "id": "FEDZRIf0uSvc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.prompts import ChatPromptTemplate\n",
        "\n",
        "template = \"\"\"You are an AI language model assistant. Your task is to generate seven\n",
        "different versions of the given user question to retrieve relevant documents from a vector\n",
        "database. By generating multiple perspectives on the user question, your goal is to help\n",
        "the user overcome some of the limitations of the distance-based similarity search.\n",
        "Provide these alternative questions separated by newlines. Original question: {question}\"\"\"\n",
        "\n",
        "prompt_perspective = ChatPromptTemplate.from_template(template)\n",
        "\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "from langchain_openai import ChatOpenAI\n",
        "\n",
        "\n",
        "generate_queries = (\n",
        "    prompt_perspective\n",
        "    | ChatOpenAI(temperature = 0)\n",
        "    | StrOutputParser()\n",
        "    | (lambda x : x.split(\"\\n\"))\n",
        ")\n",
        "\n"
      ],
      "metadata": {
        "id": "WOXnK5vbkmJn"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.load import dumps, loads\n",
        "\n",
        "\n",
        "def get_unique_docs(documents: list[list]):\n",
        "  # flatten list oof lists and convert docs to strings\n",
        "  flattened = [dumps(doc) for sublist in documents for doc in sublist]\n",
        "\n",
        "  unique_docs = list(set(flattened))\n",
        "  return [loads(doc) for doc in unique_docs]"
      ],
      "metadata": {
        "id": "yItZmUuGlDqE"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# map() is a LangChain Runnable method that:\n",
        "# Takes a list of queries\n",
        "# Performs retrieval for each query individually\n",
        "# Returns a list of lists of documents corresponding to each query\n",
        "\n",
        "chain = (\n",
        "    generate_queries\n",
        "    | retriever.map()\n",
        "    | get_unique_docs\n",
        ")\n",
        "docs = chain.invoke({\"question\": question})\n",
        "len(docs)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wh1opjXumdJM",
        "outputId": "3afad00e-1317-4bfa-a387-07dbc244e238"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "3"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from operator import itemgetter\n",
        "from langchain_openai import ChatOpenAI\n",
        "from langchain_core.runnables import RunnablePassthrough\n",
        "\n",
        "# RAG\n",
        "template = \"\"\"Answer the following question based on this context:\n",
        "\n",
        "{context}\n",
        "\n",
        "Question: {question}\n",
        "\"\"\"\n"
      ],
      "metadata": {
        "id": "SkzRiUdHnBei"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = ChatPromptTemplate.from_template(template)"
      ],
      "metadata": {
        "id": "5s7JjlAkoiYK"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "llm = ChatOpenAI(temperature=0)\n"
      ],
      "metadata": {
        "id": "STlL7_EbokV-"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "final_chain = (\n",
        "     {\"context\": chain,\n",
        "     \"question\": itemgetter(\"question\")}  ##itemgetter(\"question\") tells the pipeline to grab \"question\" from the input when invoked\n",
        "     | prompt\n",
        "     | llm\n",
        "     | StrOutputParser()\n",
        ")\n",
        "\n",
        "final_chain.invoke({\"question\": question})\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "id": "alJaxKn7ooZ7",
        "outputId": "1b211bf6-1f21-48a2-b7c5-75135eaa5aa8"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"Elizabeth seems to have a more practical and realistic view of love and courtship. She acknowledges that Lydia's behavior is shocking and lacks decency and virtue, but also recognizes that Lydia is young and has been influenced by her surroundings. Elizabeth understands that Wickham's charm and charisma can captivate a woman, but she also seems to have a more grounded perspective on the nature of love and relationships.\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "EimY5sKypEwU"
      },
      "execution_count": 36,
      "outputs": []
    }
  ]
}